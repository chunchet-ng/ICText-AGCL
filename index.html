<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="When IC Meets Text: Towards a Rich Annotated Integrated Circuit Text Dataset">
  <meta name="keywords" content="ICText, AGCL, OCR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>When IC Meets Text: Towards a Rich Annotated Integrated Circuit Text Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
  <link rel="stylesheet" href="./static/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/chip.ico">

  <script src="https://code.jquery.com/jquery-3.7.1.min.js" 
          integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
  <script defer src="./static/js/all.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" 
          integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>
  <script defer src="./static/js/index.js"></script>

</head>

<body>
  <div class="container pt-3">
    <ul class="nav nav-pills justify-content-center gap-2">
      <li class="nav-item">
        <a class="btn btn-primary" href="https://chunchet-ng.github.io/" target="_blank">
          <i class="fa-solid fa-house"></i>
            Home
        </a>
      </li>
      <li class="nav-item">
        <div class="btn-group">
          <button class="btn btn-outline-primary dropdown-toggle" type="button" 
          data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            More Research
          </button>
          <div class="dropdown-menu">
            <a class="dropdown-item" href="https://chunchet-ng.github.io/Text-in-the-Dark/" target="_blank">Text in the Dark</a>
          </div>
        </div>
      </li>
    </ul>
  </div>

  <div class="jumbotron jumbotron-fluid">
    <div class="container">
      <div class="row">
        <div class="col-2 d-none d-sm-block"></div>
        <h1 class="col-xs-12 col-md-8 text-center title">When IC Meets Text: Towards a Rich Annotated Integrated Circuit
        Text Dataset</h1>
        <div class="col-2 d-none d-sm-block"></div>
      </div>

      <div class="row">
        <div class="col-4 d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-4 text-center publication-authors">
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=1lKI2vEAAAAJ" target="_blank">Chun Chet
              Ng</a><sup>1,*</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=UewYUXwAAAAJ" target="_blank">Che-Tsung
              Lin</a><sup>2,*</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=YEwTuToAAAAJ" target="_blank">Zhi Qin
              Tan</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.linkedin.com/in/xinyu-wang-023162253/" target="_blank">Xinyu Wang</a><sup>3</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.ca/citations?hl=en&user=fYKEYmUAAAAJ" target="_blank">Jie Long
              Kew</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.ca/citations?user=hKfga9oAAAAJ" target="_blank">Chee Seng
              Chan</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.ca/citations?user=Pmi5GEAAAAAJ" target="_blank">Christopher
              Zach</a><sup>2</sup>
          </span>
        </div>
        <div class="col-4 d-none d-sm-block"></div>
      </div>

      <div class="row p-3">
        <div class="col-3 d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-6 text-center">
          <span class="author-block"><sup>1</sup>Universiti Malaya, Kuala Lumpur, Malaysia</span><br />
          <span class="author-block"><sup>2</sup>Chalmers University of Technology, Gothenburg, Sweden</span><br />
          <span class="author-block"><sup>3</sup>The University of Adelaide, Adelaide, Australia</span><br />
          <span class="author-block"><sup>*</sup>Equal Contribution</span>
        </div>
        <div class="col-3 d-none d-sm-block"></div>
      </div>

      <div class="row">
        <div class="col-3 d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-6 d-grid gap-2 d-md-block text-center">
            <a class="btn btn-outline-primary m-1">
                <i class="fa-solid fa-file-pdf"></i>
                Paper (Under Review)
            </a>
            <a class="btn btn-outline-primary m-1"
              href="https://github.com/chunchet-ng/ICText-AGCL" target="_blank">
                <i class="fa-brands fa-github"></i>
                GitHub Repo
            </a>
            <a class="btn btn-outline-primary m-1"
              href="https://github.com/chunchet-ng/ICText-AGCL#integrated-circuit-text-ictext-dataset" target="_blank">
                <i class="fa-solid fa-images"></i>
                ICText Dataset
            </a>
        </div>
        <div class="col-3 d-none d-sm-block"></div>
      </div>
    </div>
  </div>

  <div class="jumbotron jumbotron-fluid bg-light">
    <div class="container">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <h2 class="text-center title">Abstract</h1>
          <p class="text-start">Automated Optical Inspection (AOI) is a process that uses cameras to autonomously scan printed circuit
            boards for quality control. Text is often printed on chip components, and it is crucial that this text is correctly recognized during
            AOI, as it contains valuable information. In this paper, we introduce ICText, the largest dataset for text
            detection and recognition on integrated circuits. Uniquely, it includes labels for character quality attributes such as low
            contrast, blurry, and broken. While loss-reweighting and Curriculum Learning (CL) have been proposed to improve object detector
            performance by balancing positive and negative samples and gradually training the model from easy to hard samples, these methods have had
            limited success with one-stage object detectors commonly used in industry. To address this, we propose Attribute-Guided
            Curriculum Learning (AGCL), which leverages the labeled character quality attributes in ICText. Our extensive experiments demonstrate
            that AGCL can be applied to different detectors in a plug-andplay fashion to achieve higher Average Precision (AP),
            significantly outperforming existing methods on ICText without any additional computational overhead during inference. Furthermore, we
            show that AGCL is also effective on the generic object detection dataset Pascal VOC.</p>
        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>

  <div class="jumbotron jumbotron-fluid">
    <div class="container">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <h2 class="text-center title">ICText Dataset</h1>
          <p class="text-start">Samples of annotations in ICText dataset. Besides the common annotations
            (i.e. bounding box and class), we also include the multi-label character quality attributes.
            They are represented by (i) <span class="text-danger">red dot</span> for low contrast character, 
            (ii) <span class="text-success">green dot</span> dot for blurry
            characters, and lastly (iii) <span class="text-primary">blue dot</span> for broken characters.</p>
          <div class="row">
            <div class="col-md-4">
              <img
                src="static/images/59163.jpg"
                class="resize img-fluid rounded mb-2"
                alt="ICText Sample 1"
              />
            </div>

            <div class="col-md-4">
              <img
                src="static/images/91068.jpg"
                class="resize img-fluid rounded mb-2"
                alt="ICText Sample 2"
              />
            </div>
          
            <div class="col-md-4">
              <img
                src="static/images/388715.jpg"
                class="resize img-fluid rounded mb-2"
                alt="ICText Sample 3"
              />
            </div>
          </div>
          <p class="text-start"></p>The ICText dataset contains a total of 10,000 images with 100,152 legible characters. 
          Of the 69,750 annotated samples in the training set, 50,597 have quality defects (72.5%). In the testing
          set, 20,114 out of 30,402 characters (66.2%) have quality attributes. This
          indicates a high number of flawed characters in ICText, which is also the
          first-ever dataset with quality labels on characters. All
          images and annotations are fairly distributed with a 7:3 ratio between the
          training set (69.64% of complete annotations) and the testing set (30.36%). 
          Statistics of ICText are visualized in the following figures:</p>
          <div id="carouselExampleCaptions" class="carousel slide" data-bs-theme="dark">
            <div class="carousel-inner">
              <div class="carousel-item active">
                <div class="position-relative d-md-block text-secondary text-center">
                  <h5>Figure 1:</h5>
                  <p>Number of images and annotated characters (defective and perfect) in the training
                    and testing splits of ICText.</p>
                </div>
                <img src="static/images/ann_stats.png" class="d-block img-fluid">
              </div>
              <div class="carousel-item">
                <div class="position-relative text-center d-md-block text-secondary text-center">
                  <h5>Figure 2:</h5>
                  <p> Histogram of character classes in ICText.</p>
                </div>
                <img src="static/images/char_stats.png" class="d-block img-fluid">
              </div>
              <div class="carousel-item">
                <div class="position-relative text-center d-md-block text-secondary text-center">
                  <h5>Figure 3:</h5>
                  <p> Stacked bar chart of quality attributes in ICText.</p>
                </div>
                <img src="static/images/aes_stats.png" class="d-block img-fluid">
              </div>
            </div>
            <div class="position-relative text-center carousel-indicators">
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="1" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="2" aria-label="Slide 3"></button>
            </div>
            <button class="carousel-control-prev" type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>

          <p class="text-start"></p>The ICText dataset is also extended to a long-tail classification dataset, ICText-LT.
          A new Frequency Weighted Focusing and Dynamic Probability Smoothing (FFDS) loss is then proposed to dynamically
          reduces the influence of outliers and assigns class-dependent focusing parameters.
          More details can be found in this <a href="https://github.com/nwjun/FFDS-Loss" target="_blank">GitHub repo</a>.</p>

        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>

  <div class="jumbotron jumbotron-fluid bg-light">
    <div class="container">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <h2 class="text-center title">AGCL</h1>
          <p class="text-start">Attribute-Guided Curriculum Learning (AGCL) loss proposes to zero out the gradient of difficult characters.
            It can also balances the contribution of negative samples through weighting factors and focusing parameters.
            The training is split into two phases where AGCL loss is used in the first phase, and the Cross Entropy loss is used in the 
            second phase.
          </p>
          <div class="row text-center">
            <div class="col-md-12">
              <figure class="figure">
                <img src="static/images/agcl_stage.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">Schematic diagram of AGCL.</figcaption>
              </figure>
            </div>
          </div>
          <p class="text-start">The figure below shows the differences between common existing loss functions used by object detectors
            and our proposed AGCL on positive and negative cases during training.
          </p>
          <div class="row text-center">
            <div class="col-md-12">
              <figure class="figure">
                <img src="static/images/loss_compare.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">Comparison of existing loss functions and AGCL.</figcaption>
              </figure>
            </div>
          </div>
        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>

  <div class="jumbotron jumbotron-fluid">
    <div class="container">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <h2 class="text-center title">Experiment Results</h1>
          <p class="text-start">Quantitative results of all methods on ICText's test set are shown in the table below. 
            Models marked with * are tested on a subset of easier images. Both inference speed and GPU memory are tested on Titan
            X, and the rest of the hardware specifications can be found in our paper. ± marks the
            standard deviation calculated over five runs, and ↑ shows the relative AP improvement of
            AGCL-enabled methods over the baseline methods.</p>
          <div class="row text-center">
            <div class="col-md-12">
              <figure class="figure">
                <img src="static/images/quanti_res.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">Quantitative result table.</figcaption>
              </figure>
            </div>
          </div>
          <p class="text-start">Qualitative results of ABCNet, PAN++, YOLOv4 (baseline), and our proposed
            YOLOv4-AGCL are shown in the figures below. <span class="text-success">Green boxes</span> = true positives; 
            <span class="text-danger">Red boxes</span> = false positives; 
            <span class="text-primary">Blue boxes</span> = false negatives. 
            The character class prediction is shown in the top left corner of each box.</p>
          <div class="row text-center">
            <div class="col-md-6">
              <figure class="figure">
                <img src="static/images/389836_abc.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">(a) ABCNet.</figcaption>
              </figure>
            </div>

            <div class="col-md-6">
              <figure class="figure">
                <img src="static/images/389836_panpp.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">(b) PAN++.</figcaption>
              </figure>
            </div>
          </div>
          <div class="row text-center">
            <div class="col-md-6">
              <figure class="figure">
                <img src="static/images/389836_baseline.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">(c) YOLOv4 (baseline).</figcaption>
              </figure>
            </div>

            <div class="col-md-6">
              <figure class="figure">
                <img src="static/images/389836_agcl.png" class="figure-img img-fluid rounded">
                <figcaption class="figure-caption">(d) YOLOv4-AGCL (Ours).</figcaption>
              </figure>
            </div>
          </div>

          <p class="text-start">We show that asking non-AGCL detectors to learn directly from flawed characters has side effects,
          i.e., more false positives and false negatives. Additionally, both ABCNet and PAN++ suffer from 
          granularity issues. In contrast, there are significantly fewer false-positive and false-negative boxes 
          in our proposed method (i.e., YOLOv4-AGCL), showing that training a detector in an
          easy-to-hard fashion guided by quality attributes can achieve better results.</p>
        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>

  <div class="jumbotron jumbotron-fluid bg-light">
    <div class="container">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <h2 class="text-center title">BibTeX</h1>
          <p class="text-start">If you wish to cite the ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic
            Assessment paper:</p>
            <div class="text-primary border border-primary rounded p-3" id="bibtex_div">
              <p class="m-0">
                <button class="btn btn-primary float-end ms-2" onclick="copy_text()" id="bibtex_btn">
                  <div id="icon_div"><i class="fa-solid fa-copy"></i></div>
                </button>
                <span id="bibtex">
                @inproceedings{icdar2021_ictext,<br/>
                  <span class="tab"></span>author= {Ng, Chun Chet and Nazaruddin, Akmalul Khairi Bin and Lee, Yeong Khang and Wang, Xinyu
                  <span class="tab"></span>and Liu, Yuliang and Chan, Chee Seng and Jin, Lianwen and Sun, Yipeng and Fan, Lixin},<br/>
                  <span class="tab"></span>title= {ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment},<br/>
                  <span class="tab"></span>booktitle= {Document Analysis and Recognition -- ICDAR 2021},<br/>
                  <span class="tab"></span>year= {2021},<br/>
                  <span class="tab"></span>publisher= {Springer International Publishing},<br/>
                  <span class="tab"></span>pages= {663--677}}
                </span>
              </p>
            </div>
            <p class="text-start pt-3">If you wish to cite the lastest version of the ICText dataset and AGCL:</p>
              <div class="text-primary border border-primary rounded p-3" id="bibtex_div">
                <p class="m-0">
                  <button class="btn btn-primary float-end ms-2" onclick="copy_text()" id="bibtex_btn">
                    <div id="icon_div"><i class="fa-solid fa-copy"></i></div>
                  </button>
                  <span id="bibtex">
                    Our paper is currently under review. We will update this section when it is published.
                  </span>
                </p>
              </div>
        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>

</body>

<footer>
  <div class="jumbotron jumbotron-fluid">
    <div class="container text-center">
      <div class="row">
        <div class="col d-none d-sm-block"></div>
        <div class="col-xs-12 col-md-8">
          <i class="fa-regular fa-copyright"></i> 2023 Universiti Malaya. Built with Bootstrap. Inspired by 
          <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">nerfies.github.io</a>.  
        </div>
        <div class="col d-none d-sm-block"></div>
      </div>
    </div>
  </div>
</footer>

</html>