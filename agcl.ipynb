{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Tensor = torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# define random targets and predictions\n",
    "num_samples = 5\n",
    "num_classes = 2\n",
    "\n",
    "preds = torch.rand(num_samples, num_classes)\n",
    "preds_softmax = F.softmax(preds, dim=1)\n",
    "\n",
    "targets = torch.randint(0, num_classes, (num_samples, 1))\n",
    "targets_onehot = F.one_hot(targets, num_classes=num_classes).squeeze(1)\n",
    "\n",
    "preds_pos = torch.max(preds_softmax * targets_onehot, dim=1).values\n",
    "preds_neg = torch.max(preds_softmax * (1 - targets_onehot), dim=1).values\n",
    "\n",
    "# define random aesthetic classes\n",
    "aesthetics = torch.randint(0, 2, (num_samples, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set which aesthetic class/classes we want to ignore\n",
    "# the sequence is low contrast, blurry, broken\n",
    "aes_to_ignore = torch.tensor([1, 0, 0])\n",
    "\n",
    "difficult_labels = []\n",
    "for x in aesthetics:\n",
    "    if (x & aes_to_ignore).sum() > 0:\n",
    "        difficult_labels.append(1)\n",
    "    else:\n",
    "        difficult_labels.append(0)\n",
    "difficult_labels = torch.tensor(difficult_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined necessary loss functions for AGCL\n",
    "def cross_entropy(preds: Tensor = None, targets: Tensor = None):\n",
    "    \"\"\"\n",
    "    Calculate the Cross Entropy Loss given predictions and targets (ground truths).\n",
    "    Please refer to https://pytorch.org/docs/stable/generated/F.cross_entropy.html.\n",
    "\n",
    "    Args:\n",
    "        preds (Tensor): Predicted unnormalized logits.\n",
    "        targets (Tensor) : Ground truth class indices or class probabilities.\n",
    "    Returns:\n",
    "        ce_loss (Tensor): calculated cross entropy loss.\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(preds, targets.squeeze(1), reduction=\"none\")\n",
    "    return ce_loss\n",
    "\n",
    "\n",
    "def positive_agcl(\n",
    "    positive_w: float = 0.0, positive_gamma: float = 0.0, preds_pos: Tensor = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the AGCL given positive predictions and targets (ground truths).\n",
    "\n",
    "    Args:\n",
    "        positive_w (float): Value for w_+ in Equation 5.\n",
    "        positive_gamma (float): Value for gamma_+ in Equation 5.\n",
    "        preds_pos (Tensor): Predicted unnormalized logits of positive samples.\n",
    "    Returns:\n",
    "        loss (Tensor): calculated positive_agcl.\n",
    "    \"\"\"\n",
    "    loss = (\n",
    "        -positive_w * torch.pow((1 - preds_pos), positive_gamma) * torch.log(preds_pos)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def negative_agcl(\n",
    "    negative_w: float = 0.0,\n",
    "    m: float = 0.0,\n",
    "    negative_gamma: float = 0.0,\n",
    "    preds_neg: Tensor = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the AGCL given negative predictions and targets (ground truths).\n",
    "\n",
    "    Args:\n",
    "        negative_w (float): Value for w_- in Equation 5.\n",
    "        m (float): Value for shifted probability (p_m) specified in \n",
    "        the work of Asymmetric Loss (https://arxiv.org/pdf/2009.14119.pdf).\n",
    "        negative_gamma (float): Value for gamma_- in Equation 5.\n",
    "        preds_neg (Tensor): Predicted unnormalized logits of negative samples.\n",
    "    Returns:\n",
    "        loss (Tensor): calculated negative_agcl.\n",
    "    \"\"\"\n",
    "    p_m = (preds_neg - m).clamp(min=0)\n",
    "    p_m_2 = (1 - preds_neg + m).clamp(min=0)\n",
    "    loss = -negative_w * torch.pow(p_m, negative_gamma) * torch.log(p_m_2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def agcl(\n",
    "    stage: int = 1,\n",
    "    positive_w: float = 0.0,\n",
    "    positive_gamma: float = 0.0,\n",
    "    preds_pos: float = 0.0,\n",
    "    difficult_labels: List[int] = [0],\n",
    "    negative_w: float = 0.0,\n",
    "    m: float = 0.0,\n",
    "    negative_gamma: float = 0.0,\n",
    "    preds_neg: Tensor = None,\n",
    "    preds: Tensor = None,\n",
    "    targets: Tensor = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the complete AGCL in two stages.\n",
    "    Stage 1 of AGCL: \n",
    "        - Calculate the loss of positive samples using positive_agcl.\n",
    "        - Calculate the loss of negative samples using negative_agcl.\n",
    "        - Mask out the loss for difficult samples to zero out their gradients.\n",
    "    \n",
    "    Stage 2 of AGCL:\n",
    "        - Calculate the loss of all samples using cross_entropy.\n",
    "\n",
    "    Args:\n",
    "        stage (int): Value to indicate stage of AGCL.\n",
    "        positive_w (float): Value for w_+ in Equation 5.\n",
    "        positive_gamma (float): Value for gamma_+ in Equation 5.\n",
    "        preds_pos (Tensor): Predicted unnormalized logits of positive samples.\n",
    "        difficult_labels (List[int]): Mask to zero out the gradient of difficult samples.\n",
    "        negative_w (float): Value for w_- in Equation 5.\n",
    "        m (float): Value for shifted probability (p_m) specified in \n",
    "        the work of Asymmetric Loss (https://arxiv.org/pdf/2009.14119.pdf).\n",
    "        negative_gamma (float): Value for gamma_- in Equation 5.\n",
    "        preds_neg (Tensor): Predicted unnormalized logits of negative samples.\n",
    "    Returns:\n",
    "        loss (Tensor): calculated loss averaged over all samples.\n",
    "    \"\"\"\n",
    "    if stage == 1:\n",
    "        positive_loss = positive_agcl(positive_w, positive_gamma, preds_pos)\n",
    "        negative_loss = negative_agcl(negative_w, m, negative_gamma, preds_neg)\n",
    "        loss = positive_loss * difficult_labels + negative_loss * difficult_labels\n",
    "    else:\n",
    "        loss = cross_entropy(preds, targets)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we can see that loss values of tensor([0.0000, 0.7183, 1.3946, 0.0000, 1.5419]) at 0, 3 is 0.\n",
      "Therefore, we ignored the training loss of difficult characters.\n",
      "Stage 1 of AGCL Loss: 0.7309598326683044\n"
     ]
    }
   ],
   "source": [
    "# Detailed breakdown of Stage 1 of AGCL\n",
    "positive_w = 1\n",
    "positive_gamma = 0\n",
    "\n",
    "stage_1_positive_agcl = positive_agcl(positive_w, positive_gamma, preds_pos)\n",
    "\n",
    "negative_w = 0.52\n",
    "m = 0.14\n",
    "negative_gamma = 1.06\n",
    "\n",
    "stage_1_negative_agcl = negative_agcl(negative_w, m, negative_gamma, preds_neg)\n",
    "\n",
    "stage_1_final = (\n",
    "    stage_1_positive_agcl * difficult_labels + stage_1_negative_agcl * difficult_labels\n",
    ")\n",
    "\n",
    "zero_indices = \", \".join(str(x[0]) for x in (difficult_labels == 0).nonzero().tolist())\n",
    "print(f\"Here we can see that loss values of {stage_1_final} at {zero_indices} is 0.\")\n",
    "print(\"Therefore, we ignored the training loss of difficult characters.\")\n",
    "print(f\"Stage 1 of AGCL Loss: {stage_1_final.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 of AGCL Loss: 1.0470192432403564\n"
     ]
    }
   ],
   "source": [
    "# Detailed breakdown of Stage 2 of AGCL\n",
    "stage_2_ce = cross_entropy(preds, targets)\n",
    "\n",
    "print(f\"Stage 2 of AGCL Loss: {stage_2_ce.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 1, Epoch: 0, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 1, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 2, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 3, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 4, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 5, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 6, Loss: 0.7309598326683044\n",
      "Stage: 1, Epoch: 7, Loss: 0.7309598326683044\n",
      "Stage: 2, Epoch: 8, Loss: 1.0470192432403564\n",
      "Stage: 2, Epoch: 9, Loss: 1.0470192432403564\n"
     ]
    }
   ],
   "source": [
    "# General training flow of AGCL\n",
    "# This is just for demo purposes\n",
    "positive_w = 1\n",
    "positive_gamma = 0\n",
    "negative_w = 0.52\n",
    "m = 0.14\n",
    "negative_gamma = 1.06\n",
    "\n",
    "total_epochs = 10\n",
    "stage_2_epoch = 8\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch <= stage_2_epoch - 1:\n",
    "        loss = agcl(\n",
    "            1,\n",
    "            positive_w,\n",
    "            positive_gamma,\n",
    "            preds_pos,\n",
    "            difficult_labels,\n",
    "            negative_w,\n",
    "            m,\n",
    "            negative_gamma,\n",
    "            preds_neg,\n",
    "        )\n",
    "        print(f\"Stage: 1, Epoch: {epoch}, Loss: {loss}\")\n",
    "    else:\n",
    "        loss = agcl(2, preds=preds, targets=targets)\n",
    "        print(f\"Stage: 2, Epoch: {epoch}, Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "57e93ad43344a5b2c91f0d5e171bd87abae47a951db6acbe8c9234e68b9bc95b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
